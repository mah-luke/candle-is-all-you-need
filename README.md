# Candle is all you need
Hard fork of [transforming attention](https://github.com/mkleinegger/transforming-attention), a university project for the course Deep Learning for Natural Language Processing at TU Wien in WS2024, where we reproduced the results of the "Attention is all you need" paper by reimplementing the transformer architecture in both PyTorch and Candle.

